# Install required libraries 
%pip install gensim scikit-learn matplotlib

# Import libraries 
import gensim.downloader as api 
import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.decomposition import PCA 
from sklearn.manifold import TSNE

# Load pre-trained word vectors 
print("Loading pre-trained word vectors...") 
word_vectors = api.load("word2vec-google-news-300")  # Load Word2Vec model 

# Select 10 words from a specific domain (e.g., technology) 
domain_words = [
    "computer", "software", "hardware", "algorithm", "data",
    "network", "programming", "machine", "learning", "artificial"
]

# Get vectors for the selected words 
domain_vectors = np.array([word_vectors[word] for word in domain_words])

# Function to visualize word embeddings using PCA or t-SNE 
def visualize_word_embeddings(words, vectors, method): 
    if method == 'pca': 
        reducer = PCA(n_components=2) 
    elif method == 'tsne': 
        reducer = TSNE(n_components=2, random_state=42, perplexity=3) 
    else: 
        raise ValueError("Method must be 'pca' or 'tsne'.") 
    
    reduced_vectors = reducer.fit_transform(vectors) 
    
    plt.figure(figsize=(6, 4)) 
    plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], marker='o', color='blue')
    for i, word in enumerate(words): 
        plt.text(reduced_vectors[i, 0] + 0.02, reduced_vectors[i, 1] + 0.02, word, fontsize=6) 
    
    plt.title(f"Word Embeddings Visualization using {method.upper()}") 
    plt.xlabel("Component 1") 
    plt.ylabel("Component 2") 
    plt.grid(True) 
    plt.show()

# Visualize using PCA 
visualize_word_embeddings(domain_words, domain_vectors, method='pca') 

# Visualize using t-SNE 
visualize_word_embeddings(domain_words, domain_vectors, method='tsne')